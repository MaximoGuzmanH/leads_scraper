# leads_scraper
Scrapers en Python para extraer datos de empresas en Procore Network. links.py obtiene enlaces de perfiles y los guarda en MongoDB. data.py extrae detalles de cada empresa y actualiza los registros. Usa Selenium, BeautifulSoup y pymongo para automatizar el scraping y almacenamiento de datos.

# ğŸ—ï¸ Procore Network Scraper

Este repositorio contiene dos scrapers para extraer informaciÃ³n de empresas del sitio web [Procore Network](https://www.procore.com/network). Se utilizan `Selenium` y `BeautifulSoup` para navegar en la web y obtener datos estructurados de cada empresa.

## ğŸ“Œ CaracterÃ­sticas

âœ… **Scraper de enlaces (`links.py`)**:  
- Navega en Procore Network por estado.
- Aplica filtros de empresas contratistas generales y especializadas.
- Extrae y almacena enlaces en MongoDB.

âœ… **Scraper de datos (`data.py`)**:  
- Procesa los enlaces guardados en MongoDB.
- Extrae informaciÃ³n clave de cada empresa.
- Ejecuta mÃºltiples solicitudes en paralelo para mayor eficiencia.

## ğŸ“¦ Requisitos

Antes de ejecutar los scrapers, asegÃºrate de tener instalados los siguientes paquetes:

```sh
pip install selenium beautifulsoup4 requests pymongo webdriver-manager
```


ğŸš€ Uso
1ï¸âƒ£ Ejecutar el scraper de enlaces
sh
Copiar
Editar
python links.py
Este proceso recorrerÃ¡ cada estado en la lista y guardarÃ¡ los enlaces de empresas en MongoDB.

2ï¸âƒ£ Ejecutar el scraper de datos
sh
Copiar
Editar
python data.py
Este script tomarÃ¡ los enlaces almacenados y extraerÃ¡ informaciÃ³n detallada de cada empresa.

âš™ï¸ ConfiguraciÃ³n Adicional
ğŸ”¹ Modo sin cabeza (headless) en Selenium:
El scraper usa headless mode por defecto. Si quieres ver la ejecuciÃ³n en un navegador, edita links.py y comenta la opciÃ³n:

python
Copiar
Editar
# options.add_argument("--headless")
ğŸ”¹ NÃºmero de hilos en data.py:
Puedes cambiar la cantidad de hilos para mejorar el rendimiento:

python
Copiar
Editar
if __name__ == "__main__":
    scrape_business_data(max_workers=10)  # Ajusta el nÃºmero segÃºn tu CPU
ğŸ› ï¸ TecnologÃ­as Usadas
Selenium: NavegaciÃ³n automatizada
BeautifulSoup: ExtracciÃ³n de datos HTML
Requests: Peticiones HTTP
MongoDB: Almacenamiento de datos
Concurrent Futures: Procesamiento paralelo
ğŸ“ Notas
Evita bloqueos: Usa HEADERS en data.py para simular un navegador real.
Manejo de errores: Si una pÃ¡gina no carga, el scraper ignora el registro y continÃºa con el siguiente.
Persistencia de datos: La base de datos MongoDB mantiene el estado de los registros (status=0 para pendientes, status=1 para completados).
ğŸ“„ Licencia
Este proyecto estÃ¡ disponible bajo la licencia MIT.

ğŸ“© Â¡Contribuciones y mejoras son bienvenidas! Si encuentras algÃºn problema o tienes una sugerencia, abre un issue en este repositorio.
